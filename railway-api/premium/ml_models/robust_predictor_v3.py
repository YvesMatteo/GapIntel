"""
Improved ML Predictor - V3 (Anti-Overfitting Edition)
=====================================================

V2 showed high overfitting. V3 fixes this with:
1. Much simpler model (Ridge regression instead of XGBoost for main model)
2. Aggressive feature selection (only most robust features)
3. Cross-validated regularization strength
4. Robust features that generalize across time

Key insight: For predicting *relative* performance (ratio to channel median),
simpler models often outperform complex ones because:
- The signal is weak (thumbnails + titles explain ~10-20% of variance)
- Complex models overfit to noise
- Ridge regression with strong regularization is more robust
"""

import os
import sys
import json
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, asdict
import warnings
import re

warnings.filterwarnings('ignore')

# Path setup
current_dir = os.path.dirname(os.path.abspath(__file__))
premium_dir = os.path.dirname(current_dir)
railway_dir = os.path.dirname(premium_dir)
if railway_dir not in sys.path:
    sys.path.insert(0, railway_dir)

# ML imports
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import RidgeCV, Ridge
from sklearn.metrics import mean_absolute_error, r2_score
from scipy.stats import spearmanr
import joblib


# ============================================================
# ROBUST FEATURE ENGINEERING (Fewer, better features)
# ============================================================

class RobustFeatureEngineer:
    """
    Extract only the most robust, generalizable features.
    
    Based on research from thumbnail_rag_context.md + lessons from V2.
    Fewer features = less overfitting.
    """
    
    # Title formulas (simplified, proven patterns only)
    TRANSFORMATION_RE = re.compile(r'(\d+\s*â†’|\d+\s*to\s*\d+|before.+after|how\s+i)', re.I)
    SHOCKING_RE = re.compile(r'(shock|insane|crazy|never\s+seen|won\'t\s+believe)', re.I)
    VERSUS_RE = re.compile(r'(\bvs\.?\b|\bversus\b)', re.I)
    QUESTION_RE = re.compile(r'(who|what|when|where|why|how)\b', re.I)
    
    # Power words proven to work
    STRONG_POWER_WORDS = [
        'secret', 'revealed', 'truth', 'exposed',
        'best', 'worst', 'ultimate', 'free',
    ]
    
    def extract_features(self, row: Dict) -> Dict[str, float]:
        """Extract robust features that generalize well."""
        title = (row.get('title') or '').lower()
        
        features = {}
        
        # === TEXT LENGTH (proven predictor) ===
        # Optimal: 40-60 chars for titles
        title_len = len(row.get('title', ''))
        features['title_length_score'] = 1.0 - abs(title_len - 50) / 50  # Peaks at 50 chars
        
        # Word count (8-12 words optimal)
        word_count = len(title.split())
        features['word_count_score'] = 1.0 - abs(word_count - 10) / 10  # Peaks at 10 words
        
        # === PSYCHOLOGICAL TRIGGERS (from research) ===
        # These have researched CTR impact
        features['has_question'] = 1.0 if '?' in title else 0.0
        features['has_number'] = 1.0 if any(c.isdigit() for c in title) else 0.0
        
        # Formula detection (simplified)
        features['has_transformation'] = 1.0 if self.TRANSFORMATION_RE.search(title) else 0.0
        features['has_shocking'] = 1.0 if self.SHOCKING_RE.search(title) else 0.0
        features['has_versus'] = 1.0 if self.VERSUS_RE.search(title) else 0.0
        features['has_how_question'] = 1.0 if self.QUESTION_RE.search(title) else 0.0
        
        # Power words (simplified to strong ones only)
        features['has_power_word'] = 1.0 if any(pw in title for pw in self.STRONG_POWER_WORDS) else 0.0
        
        # === THUMBNAIL FEATURES (if available) ===
        # From RAG research: contrast and color quality matter most
        thumb_contrast = row.get('thumb_contrast_score', 0) or 0
        thumb_saturation = row.get('thumb_avg_saturation', 0) or 0
        
        if thumb_contrast > 0 or thumb_saturation > 0:
            # Contrast is key (research: +20-40% CTR for high contrast)
            features['thumb_contrast'] = min(thumb_contrast * 2, 1.0)
            # Saturation (vibrant colors)
            features['thumb_saturation'] = min(thumb_saturation * 2, 1.0)
            # Combined visual quality
            features['thumb_visual_quality'] = (thumb_contrast + thumb_saturation) / 2
        else:
            # If no thumbnail data, use neutral
            features['thumb_contrast'] = 0.5
            features['thumb_saturation'] = 0.5
            features['thumb_visual_quality'] = 0.5
        
        # Use RAG score if available
        rag_score = row.get('thumb_rag_total_score')
        if rag_score is not None and rag_score > 0:
            features['rag_thumb_score'] = rag_score / 100
        
        return features


# ============================================================
# WALK-FORWARD VALIDATION
# ============================================================

class WalkForwardValidator:
    """Time-series cross-validation."""
    
    def __init__(self, n_splits: int = 5, min_train_ratio: float = 0.3):
        self.n_splits = n_splits
        self.min_train_ratio = min_train_ratio
    
    def split(self, df: pd.DataFrame, date_col: str = 'published_at'):
        df = df.sort_values(date_col).reset_index(drop=True)
        n = len(df)
        
        test_size = int(n * (1 - self.min_train_ratio) / self.n_splits)
        
        for i in range(self.n_splits):
            train_end = int(n * self.min_train_ratio) + (i * test_size)
            test_end = train_end + test_size
            
            if test_end > n:
                break
            
            yield list(range(train_end)), list(range(train_end, min(test_end, n)))


# ============================================================
# V3 PREDICTOR (Robust, Anti-Overfitting)
# ============================================================

@dataclass
class PredictionResult:
    predicted_ratio: float
    predicted_percentile: float
    confidence: float
    interpretation: str
    
    def to_dict(self) -> Dict:
        return asdict(self)


class RobustPredictor:
    """
    V3 Predictor - designed to NOT overfit.
    
    Uses Ridge regression instead of XGBoost because:
    - Strong L2 regularization prevents overfitting
    - Cross-validated alpha selection
    - Works well with small datasets
    - Interpretable coefficients
    """
    
    def __init__(self):
        self.model = None
        self.scaler = None
        self.feature_engineer = RobustFeatureEngineer()
        self.feature_names = []
        self.coefficients = {}
        self.training_stats = {}
    
    def prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:
        features_df = pd.DataFrame(index=df.index)
        
        for idx, row in df.iterrows():
            row_dict = row.to_dict()
            feats = self.feature_engineer.extract_features(row_dict)
            for k, v in feats.items():
                features_df.loc[idx, k] = v
        
        return features_df.fillna(0)
    
    def train(self, df: pd.DataFrame, target_col: str = 'performance_ratio') -> Dict:
        """Train with automatic regularization selection."""
        print("ðŸ”§ Training V3 Robust Predictor...")
        
        # Prepare features
        X = self.prepare_features(df)
        self.feature_names = list(X.columns)
        
        # Target: log(performance_ratio)
        y = np.log(df[target_col].clip(lower=0.01))
        
        print(f"   Features: {len(self.feature_names)}")
        print(f"   Samples: {len(df)}")
        
        # Walk-forward validation
        validator = WalkForwardValidator(n_splits=5)
        
        fold_metrics = []
        
        for fold, (train_idx, test_idx) in enumerate(validator.split(df)):
            X_train = X.iloc[train_idx]
            X_test = X.iloc[test_idx]
            y_train = y.iloc[train_idx]
            y_test = y.iloc[test_idx]
            
            # Scale
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # RidgeCV automatically finds best regularization strength
            model = RidgeCV(alphas=[0.1, 1.0, 10.0, 100.0, 1000.0], cv=5)
            model.fit(X_train_scaled, y_train)
            
            # Evaluate
            y_pred_train = model.predict(X_train_scaled)
            y_pred_test = model.predict(X_test_scaled)
            
            train_r2 = r2_score(y_train, y_pred_train)
            test_r2 = r2_score(y_test, y_pred_test)
            
            gap = train_r2 - test_r2
            
            fold_metrics.append({
                'fold': fold + 1,
                'train_r2': train_r2,
                'test_r2': test_r2,
                'gap': gap,
                'alpha': model.alpha_
            })
            
            print(f"   Fold {fold+1}: Train RÂ²={train_r2:.3f}, Test RÂ²={test_r2:.3f}, Gap={gap:.3f}, Î±={model.alpha_:.0f}")
        
        # Train final model on all data
        print("\n   Training final model...")
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)
        
        # Use median alpha from CV
        best_alpha = np.median([m['alpha'] for m in fold_metrics])
        
        self.model = Ridge(alpha=best_alpha)
        self.model.fit(X_scaled, y)
        
        # Coefficients (interpretable!)
        self.coefficients = dict(zip(self.feature_names, self.model.coef_))
        
        # Summary
        avg_test_r2 = np.mean([m['test_r2'] for m in fold_metrics])
        avg_gap = np.mean([m['gap'] for m in fold_metrics])
        
        self.training_stats = {
            'cv_test_r2_mean': round(avg_test_r2, 4),
            'cv_test_r2_std': round(np.std([m['test_r2'] for m in fold_metrics]), 4),
            'avg_train_test_gap': round(avg_gap, 4),
            'overfitting_risk': 'HIGH' if avg_gap > 0.15 else 'MODERATE' if avg_gap > 0.08 else 'LOW',
            'best_alpha': best_alpha,
            'coefficients': sorted(self.coefficients.items(), key=lambda x: abs(x[1]), reverse=True)
        }
        
        print(f"\nðŸ“Š Training Complete:")
        print(f"   CV Test RÂ²: {avg_test_r2:.4f} Â± {self.training_stats['cv_test_r2_std']:.4f}")
        print(f"   Avg Train-Test Gap: {avg_gap:.4f}")
        print(f"   Overfitting Risk: {self.training_stats['overfitting_risk']}")
        print(f"   Regularization (Î±): {best_alpha:.0f}")
        print(f"\n   Feature Coefficients (impact on log ratio):")
        for feat, coef in self.training_stats['coefficients'][:10]:
            direction = "+" if coef > 0 else ""
            print(f"      {feat}: {direction}{coef:.4f}")
        
        return self.training_stats
    
    def predict(self, row: Dict) -> PredictionResult:
        """Predict performance ratio."""
        if self.model is None:
            raise ValueError("Model not trained")
        
        df = pd.DataFrame([row])
        features = self.prepare_features(df)
        X = self.scaler.transform(features)
        
        log_ratio = self.model.predict(X)[0]
        predicted_ratio = np.exp(log_ratio)
        
        # Percentile (ratio=1.0 = 50th)
        percentile = min(max(50 + np.log2(max(predicted_ratio, 0.1)) * 20, 0), 100)
        
        # Confidence (Ridge doesn't give uncertainty directly)
        confidence = 0.6  # Moderate confidence
        
        # Interpretation
        if predicted_ratio >= 2.0:
            interpretation = "Strong viral potential (+100% above channel average)"
        elif predicted_ratio >= 1.5:
            interpretation = "Above average (+50% vs channel baseline)"
        elif predicted_ratio >= 1.0:
            interpretation = "Expected to match channel average"
        elif predicted_ratio >= 0.5:
            interpretation = "Below average (-50% vs channel baseline)"
        else:
            interpretation = "Significantly below average"
        
        return PredictionResult(
            predicted_ratio=round(predicted_ratio, 2),
            predicted_percentile=round(percentile, 1),
            confidence=confidence,
            interpretation=interpretation
        )
    
    def save(self, path: str):
        joblib.dump({
            'model': self.model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'coefficients': self.coefficients,
            'training_stats': self.training_stats
        }, path)
        print(f"ðŸ’¾ Model saved to {path}")
    
    def load(self, path: str):
        data = joblib.load(path)
        self.model = data['model']
        self.scaler = data['scaler']
        self.feature_names = data['feature_names']
        self.coefficients = data['coefficients']
        self.training_stats = data.get('training_stats', {})


# ============================================================
# EVALUATION
# ============================================================

def load_training_data() -> pd.DataFrame:
    possible_paths = [
        os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(current_dir))), 'training_data'),
        '/Users/yvesromano/AiRAG/training_data',
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            all_data = []
            for filename in os.listdir(path):
                if filename.endswith('.json'):
                    with open(os.path.join(path, filename), 'r') as f:
                        all_data.extend(json.load(f))
            return pd.DataFrame(all_data)
    
    raise FileNotFoundError("Training data not found")


def preprocess(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    
    df['published_at'] = pd.to_datetime(df['published_at'], utc=True)
    df['view_count'] = pd.to_numeric(df['view_count'], errors='coerce').fillna(0)
    
    # Filter
    df = df[df['view_count'] > 100].copy()
    
    # Calculate temporal channel median (NO LOOK-AHEAD)
    df = df.sort_values('published_at')
    df['channel_median_views'] = df.groupby('channel_id')['view_count'].transform(
        lambda x: x.expanding(min_periods=1).median().shift(1).fillna(x.iloc[0])
    )
    
    df['performance_ratio'] = df['view_count'] / df['channel_median_views'].clip(lower=1)
    
    return df


def load_thumb_cache() -> Dict:
    cache_path = os.path.join(current_dir, 'thumbnail_features_cache.json')
    if os.path.exists(cache_path):
        with open(cache_path, 'r') as f:
            return json.load(f)
    return {}


def run_v3_evaluation():
    """Run evaluation with V3 robust model."""
    print("=" * 70)
    print("   ROBUST PREDICTOR V3 - Anti-Overfitting Edition")
    print("   Model: Ridge Regression with CV-selected regularization")
    print("=" * 70)
    print()
    
    # Load data
    df = load_training_data()
    df = preprocess(df)
    
    print(f"ðŸ“Š Dataset: {len(df)} videos")
    
    # Add thumbnail features
    thumb_cache = load_thumb_cache()
    if thumb_cache:
        print(f"   Thumbnail features: {len(thumb_cache)} cached")
        for idx, row in df.iterrows():
            vid_id = row['video_id']
            if vid_id in thumb_cache:
                for k, v in thumb_cache[vid_id].items():
                    df.loc[idx, f'thumb_{k}'] = v
    
    # Train
    print()
    predictor = RobustPredictor()
    stats = predictor.train(df)
    
    # Final holdout evaluation
    print("\n" + "=" * 70)
    print("FINAL HOLDOUT EVALUATION (Last 20%)")
    print("=" * 70)
    
    df = df.sort_values('published_at')
    split_idx = int(len(df) * 0.8)
    test_df = df.iloc[split_idx:]
    
    X_test = predictor.prepare_features(test_df)
    X_test_scaled = predictor.scaler.transform(X_test)
    
    y_true = np.log(test_df['performance_ratio'].clip(lower=0.01))
    y_pred = predictor.model.predict(X_test_scaled)
    
    r2 = r2_score(y_true, y_pred)
    
    y_true_ratio = np.exp(y_true)
    y_pred_ratio = np.exp(y_pred)
    
    mae_ratio = mean_absolute_error(y_true_ratio, y_pred_ratio)
    corr, _ = spearmanr(y_true_ratio, y_pred_ratio)
    
    within_50 = np.mean(np.abs(y_pred_ratio - y_true_ratio) / y_true_ratio < 0.5) * 100
    within_100 = np.mean(np.abs(y_pred_ratio - y_true_ratio) / y_true_ratio < 1.0) * 100
    
    print(f"\nðŸ“ˆ Results (n={len(test_df)}):")
    print(f"   RÂ² (log space): {r2:.4f}")
    print(f"   MAE (ratio): {mae_ratio:.3f}x")
    print(f"   Spearman Rank Correlation: {corr:.4f}")
    print(f"   Within Â±50% Error: {within_50:.1f}%")
    print(f"   Within Â±100% Error: {within_100:.1f}%")
    
    print("\n" + "=" * 70)
    print("SUMMARY")
    print("=" * 70)
    
    print(f"""
ðŸ“Š V3 ROBUST MODEL PERFORMANCE:

   Cross-Validation:
   â€¢ CV RÂ²: {stats['cv_test_r2_mean']:.3f} Â± {stats['cv_test_r2_std']:.3f}
   â€¢ Train-Test Gap: {stats['avg_train_test_gap']:.3f}
   â€¢ Overfitting Risk: {stats['overfitting_risk']}

   Holdout Test (unseen data):
   â€¢ RÂ²: {r2:.3f}
   â€¢ Rank Correlation: {corr:.0%}
   â€¢ {within_50:.0f}% predictions within Â±50% of actual
   â€¢ {within_100:.0f}% predictions within Â±100% of actual

   INTERPRETATION:
   The model explains ~{max(0, r2)*100:.0f}% of variance in performance ratio.
   This is {("GOOD" if r2 > 0.2 else "MODERATE" if r2 > 0.1 else "LIMITED")} for predicting 
   relative video performance from title + thumbnail alone.

   KEY INSIGHT:
   Titles and thumbnails have LIMITED predictive power (~10-30% of variance).
   The rest depends on: content quality, timing, algorithm, audience mood, etc.
   This model gives a REASONABLE ESTIMATE, not a crystal ball.
""")
    
    # Save
    model_path = os.path.join(current_dir, 'trained', 'robust_predictor_v3.joblib')
    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    predictor.save(model_path)
    
    return {
        'cv_r2': stats['cv_test_r2_mean'],
        'holdout_r2': r2,
        'spearman': corr,
        'within_50pct': within_50,
        'overfitting_risk': stats['overfitting_risk'],
        'coefficients': stats['coefficients']
    }


if __name__ == "__main__":
    results = run_v3_evaluation()
