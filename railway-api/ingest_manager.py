#!/usr/bin/env python3
"""
YouTube Ingest Manager - Phase 2 (Modular)

Fetches YouTube video audio, transcribes with Whisper, and extracts viewer questions.
Can be used standalone or imported by other scripts (e.g., gap_analyzer.py).

Usage (standalone):
    python3 ingest_manager.py "https://youtube.com/watch?v=VIDEO_ID"
    
Usage (as module):
    from ingest_manager import process_video
    result = process_video(url, api_key)
"""

import argparse
import os
import re
import sys
from datetime import datetime
from pathlib import Path
import json
import time
import random


# Load environment variables
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("‚ùå python-dotenv not installed. Run: pip install python-dotenv")
    sys.exit(1)

try:
    import yt_dlp
except ImportError:
    print("‚ùå yt-dlp not installed. Run: pip install yt-dlp")
    sys.exit(1)

# import whisper (Removed: strict caption-only mode)
try:
    from faster_whisper import WhisperModel
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False
    print("‚ö†Ô∏è faster-whisper not installed, local transcription disabled")


try:
    from googleapiclient.discovery import build
    from googleapiclient.errors import HttpError
except ImportError:
    print("‚ùå google-api-python-client not installed. Run: pip install google-api-python-client")
    sys.exit(1)

# Try to import youtube-transcript-api
try:
    from youtube_transcript_api import YouTubeTranscriptApi
    CAPTIONS_AVAILABLE = True
except ImportError:
    CAPTIONS_AVAILABLE = False
    print("‚ö†Ô∏è youtube-transcript-api not available, smart transcription disabled")

# Import ContentClusteringEngine for embedding pre-computation
try:
    # Handle both relative (module) and absolute imports
    try:
        from premium.ml_models.content_clusterer import ContentClusteringEngine
    except ImportError:
        sys.path.append(os.path.dirname(os.path.abspath(__file__)))
        from premium.ml_models.content_clusterer import ContentClusteringEngine
    CLUSTERING_AVAILABLE = True
except ImportError:
    CLUSTERING_AVAILABLE = False
    print("‚ö†Ô∏è ContentClusteringEngine not available, embeddings will not be pre-computed")


# Whisper model cache
_whisper_model = None
_whisper_model_name = None


def get_whisper_model(model_name: str = "tiny"):
    """Get cached Whisper model or load it."""
    global _whisper_model, _whisper_model_name
    if not WHISPER_AVAILABLE:
        return None
        
    if _whisper_model is None or _whisper_model_name != model_name:
        print(f"   üöÄ Loading Faster-Whisper {model_name} model...")
        # device="auto" handles CUDA vs CPU automatically
        _whisper_model = WhisperModel(model_name, device="auto", compute_type="int8")
        _whisper_model_name = model_name
    return _whisper_model


def extract_video_id(url: str) -> str:
    """Extract video ID from various YouTube URL formats."""
    patterns = [
        r'(?:v=|/v/|youtu\.be/)([a-zA-Z0-9_-]{11})',
        r'(?:embed/)([a-zA-Z0-9_-]{11})',
        r'(?:shorts/)([a-zA-Z0-9_-]{11})',
    ]
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    raise ValueError(f"Could not extract video ID from URL: {url}")


def sanitize_filename(title: str) -> str:
    """Sanitize video title for use as filename."""
    sanitized = re.sub(r'[<>:"/\\|?*]', '', title)
    sanitized = re.sub(r'\s+', '_', sanitized)
    return sanitized[:100]


def clear_temp_data(temp_dir: Path = None):
    """Clean up temporary audio and data files."""
    if temp_dir is None:
        script_dir = Path(__file__).parent.resolve()
        temp_dir = script_dir / "data" / ".temp"
    
    if temp_dir.exists() and temp_dir.is_dir():
        import shutil
        print(f"   üßπ Clearing temporary data in {temp_dir}...")
        for item in temp_dir.iterdir():
            try:
                if item.is_file():
                    item.unlink()
                elif item.is_dir():
                    shutil.rmtree(item)
            except Exception as e:
                print(f"      ‚ö†Ô∏è Failed to delete {item.name}: {e}")


def fetch_all_comments(video_id: str, api_key: str, max_comments: int = 500) -> list[dict]:
    """
    Fetch ALL comments from a YouTube video (no filtering).
    Returns raw comments for AI processing, sorted by relevance (top comments first).
    """
    youtube = build('youtube', 'v3', developerKey=api_key)
    
    comments = []
    next_page_token = None
    
    try:
        while len(comments) < max_comments:
            request = youtube.commentThreads().list(
                part='snippet',
                videoId=video_id,
                maxResults=100,
                pageToken=next_page_token,
                textFormat='plainText',
                order='relevance'
            )
            response = request.execute()
            
            for item in response.get('items', []):
                snippet = item['snippet']['topLevelComment']['snippet']
                comments.append({
                    'author': snippet['authorDisplayName'],
                    'text': snippet['textDisplay'],
                    'likes': snippet['likeCount'],
                    'published_at': snippet['publishedAt'],
                })
            
            next_page_token = response.get('nextPageToken')
            if not next_page_token:
                break
                
    except HttpError as e:
        if e.resp.status == 403:
            print(f"   ‚ö†Ô∏è Comments disabled or quota exceeded")
        else:
            print(f"   ‚ö†Ô∏è API error: {e}")
    
    return comments


def download_audio(youtube_url: str, output_dir: Path, video_id: str, verbose: bool = False, max_retries: int = 3) -> tuple[Path, dict]:
    """
    Download audio from YouTube video using yt-dlp with robust fallbacks.
    
    Implements multiple strategies to handle YouTube blocks:
    1. Exponential backoff retries
    2. Different format fallbacks
    3. Anti-blocking measures (user-agent rotation, etc.)
    
    Returns:
        Tuple of (audio_path, video_info)
    """
    import random
    import time
    
    audio_path = output_dir / f"{video_id}.mp3"
    
    # Force ffmpeg directory into PATH (Railway environment fix)
    os.environ["PATH"] = os.environ.get("PATH", "") + os.pathsep + "/usr/bin"
    
    # User agents to rotate through
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    ]
    
    # Different format strategies to try
    format_strategies = [
        'bestaudio/best',
        'worstaudio/worst',  # Sometimes simpler formats work better
        'bestaudio[ext=m4a]/bestaudio/best',
        '140',  # m4a audio format code
    ]
    
    last_error = None
    
    for attempt in range(max_retries):
        # Rotate through strategies
        format_choice = format_strategies[attempt % len(format_strategies)]
        user_agent = random.choice(user_agents)
        
        if verbose or attempt > 0:
            print(f"   üì• Download attempt {attempt + 1}/{max_retries} (format: {format_choice[:15]}...)")
        
        ydl_opts = {
            'format': format_choice,
            'outtmpl': str(output_dir / f"{video_id}.%(ext)s"),
            'quiet': True,
            'no_warnings': True,
            'ffmpeg_location': '/usr/bin/',
            # Timeout and retry settings
            'socket_timeout': 60,
            'retries': 5,
            'fragment_retries': 5,
            'http_chunk_size': 10485760,  # 10MB chunks
            # Anti-blocking measures
            'user_agent': user_agent,
            'referer': 'https://www.youtube.com/',
            'sleep_interval': 1,  # Small delay between requests
            'max_sleep_interval': 3,
            # Bypass age gate and other restrictions
            'nocheckcertificate': True,
            'ignoreerrors': False,
            # Try to extract video without downloading first (faster failure detection)
            'skip_download': False,
            # Use cookies from browser if available
            'cookiesfrombrowser': None,  # Can be set to ('chrome',) etc.
            # Extractor retries
            'extractor_retries': 3,
        }
        
        try:
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                info = ydl.extract_info(youtube_url, download=True)
                
                # Find the downloaded file
                downloaded_files = list(output_dir.glob(f"{video_id}.*"))
                if not downloaded_files:
                    raise FileNotFoundError(f"Download completed but file not found for {video_id}")
                
                audio_path = downloaded_files[0]
                
                video_info = {
                    'id': info.get('id'),
                    'video_id': info.get('id'),  # Add for compatibility
                    'title': info.get('title', 'Unknown Title'),
                    'channel': info.get('channel', 'Unknown Channel'),
                    'duration': info.get('duration'),
                    'upload_date': info.get('upload_date'),
                    'view_count': info.get('view_count'),
                    'like_count': info.get('like_count'),
                    'thumbnail_url': info.get('thumbnail'),
                    'description': info.get('description', '')[:500],
                    'url': youtube_url,
                }
                
                if verbose:
                    print(f"   ‚úì Downloaded successfully: {video_info['title'][:40]}...")
                
                return audio_path, video_info
                
        except Exception as e:
            last_error = e
            error_str = str(e).lower()
            
            # Check if it's a permanent error (video unavailable, private, etc.)
            permanent_errors = ['private video', 'video unavailable', 'removed', 'terminated', 'copyright']
            if any(err in error_str for err in permanent_errors):
                if verbose:
                    print(f"   ‚ö†Ô∏è Video unavailable (permanent): {str(e)[:80]}")
                break
            
            # For temporary errors (403, rate limit), wait and retry
            if attempt < max_retries - 1:
                wait_time = (2 ** attempt) + random.uniform(1, 3)  # Exponential backoff with jitter
                if verbose:
                    print(f"   ‚ö†Ô∏è Failed: {str(e)[:50]}... Retrying in {wait_time:.1f}s")
                time.sleep(wait_time)
            else:
                if verbose:
                    print(f"   ‚ö†Ô∏è Failed: {str(e)[:80]}")
    
    # All retries failed - raise the last error
    raise Exception(f"Failed to download {video_id} after {max_retries} attempts: {last_error}")


def transcribe_audio(audio_path: Path, model_name: str = "tiny") -> dict:
    """
    Transcribe audio using Faster-Whisper (local).
    """
    model = get_whisper_model(model_name)
    if not model:
        return {"text": "", "language": "unknown", "segments": []}
        
    segments, info = model.transcribe(str(audio_path), beam_size=5)
    
    full_text = ""
    processed_segments = []
    
    # segments is an iterable, we need to gather them
    for segment in segments:
        full_text += segment.text + " "
        processed_segments.append({
            'start': round(segment.start, 2),
            'end': round(segment.end, 2),
            'text': segment.text.strip(),
        })
    
    return {
        'text': full_text.strip(),
        'language': info.language,
        'segments': processed_segments,
    }


def fetch_captions(video_id: str) -> dict:
    """
    Fetch YouTube captions for a video using youtube-transcript-api.
    Returns format compatible with Whisper output: {text, segments}
    """
    if not CAPTIONS_AVAILABLE:
        return None
        
    try:
        # Use new API (v1.2.3+)
        api = YouTubeTranscriptApi()
        transcript = api.fetch(video_id)
        
        full_text = ""
        segments = []
        
        for snippet in transcript:
            text = snippet.text.replace('\n', ' ').strip()
            start = snippet.start
            end = start + snippet.duration
            
            full_text += text + " "
            
            segments.append({
                'start': start,
                'end': end,
                'text': text
            })
            
        return {
            'text': full_text.strip(),
            'language': 'en',  # Assumed or API could provide
            'segments': segments
        }
    except Exception as e:
        # Don't print huge stack traces for common "no caption" errors
        error = str(e).lower()
        if 'disabled' in error or 'not found' in error or 'no transcript' in error:
            return None
        print(f"‚ö†Ô∏è Caption fetch failed: {e}")
        return None


def process_video(url: str, api_key: str, model_name: str = "tiny", 
                  temp_dir: Path = None, verbose: bool = True, max_comments: int = 200) -> dict:
    """
    Process a single YouTube video: download, transcribe, fetch comments.
    
    This is the main reusable function for other scripts.
    
    Args:
        url: YouTube video URL
        api_key: YouTube Data API key
        model_name: Whisper model size
        temp_dir: Directory for temp audio files (default: ./data/.temp)
        verbose: Print progress messages
        
    Returns:
        dict with keys:
            - video_info: Video metadata
            - transcript: Full transcript text
            - transcript_segments: List of {start, end, text}
            - comments: List of raw comments
    """
    # Setup temp directory
    if temp_dir is None:
        script_dir = Path(__file__).parent.resolve()
        temp_dir = script_dir / "data" / ".temp"
    temp_dir.mkdir(parents=True, exist_ok=True)
    
    # Extract video ID
    video_id = extract_video_id(url)
    
    # Check Cache
    cache_dir = temp_dir.parent / "cache"
    cache_dir.mkdir(parents=True, exist_ok=True)
    cache_file = cache_dir / f"{video_id}.json"
    
    if cache_file.exists():
        try:
            if verbose:
                print(f"\nüì¶ Loading from cache: {video_id}")
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
            
            # Basic validation
            if cached_data.get('video_info') and cached_data.get('transcript') is not None:
                if verbose:
                     print(f"   ‚úì Cache hit! Skipping fetch.")
                return cached_data
        except Exception as e:
            if verbose:
                print(f"   ‚ö†Ô∏è Cache read failed: {e}")
    
    if verbose:
        print(f"\nüìπ Processing: {video_id}")
    
    # Step 1: Smart Transcription (Try captions first, fallback to transcription)
    transcription = None
    video_info = None
    
    # Try fetching captions
    if verbose:
        print(f"   üîç Checking for captions...")
    
    caption_result = fetch_captions(video_id)
    
    if caption_result:
        if verbose:
            print(f"   ‚úÖ Captions found!")
        transcription = caption_result
    else:
        if verbose:
            print(f"   ‚ùå No captions found. Skipping video (Audio transcription disabled in fast mode).")
        return None
    
    # Fetch metadata (lightweight, no download)
    if verbose:
        print(f"   ‚ÑπÔ∏è Fetching metadata only...")
    
    try:
        ydl_opts_meta = {
            'quiet': True,
            'no_warnings': True,
            'skip_download': True, # Key: don't download anything
        }
        with yt_dlp.YoutubeDL(ydl_opts_meta) as ydl:
            info = ydl.extract_info(url, download=False)
            video_info = {
                'id': info.get('id'),
                'title': info.get('title'),
                'url': info.get('webpage_url'),
                'uploader': info.get('uploader'),
                'uploader_id': info.get('uploader_id'),
                'description': info.get('description'),
                'view_count': info.get('view_count'),
                'like_count': info.get('like_count'),
                'duration': info.get('duration'),
                'upload_date': info.get('upload_date'),
                'thumbnail_url': info.get('thumbnail'),
                'tags': info.get('tags', []),
            }
    except Exception as e:
        print(f"‚ö†Ô∏è Metadata fetch failed: {e}")
        # Minimal fallback
        video_info = {'id': video_id, 'title': f"Video {video_id}", 'url': url}

    # Step 2: Fetch comments
    if verbose:
        print(f"   üí¨ Fetching comments...")
    comments = fetch_all_comments(video_id, api_key, max_comments=max_comments)
    if verbose:
        print(f"   ‚úì {len(comments)} comments")
    
    word_count = len(transcription['text'].split())
    if verbose:
        print(f"   ‚úì {word_count} words in transcript")
    
    # Step 3: Pre-compute Embeddings (Async-ish)
    if CLUSTERING_AVAILABLE and video_info:
        if verbose:
            print(f"   üß† Pre-computing semantic embeddings...")
        try:
            # Initialize engine (this handles DB connection)
            engine = ContentClusteringEngine(use_embeddings=True)
            
            # Prepare video data (needs video_id, title)
            # Add description to metadata if possible for richer embeddings
            v_data = {
                'video_id': video_id,
                'title': video_info.get('title', ''),
                'description': video_info.get('description', '')
            }
            
            # This triggers check_cache -> compute -> cache_embeddings
            # We call it on a single video list
            engine._cluster_with_embeddings([v_data], n_clusters=1)
            
            if verbose:
                print(f"   ‚úì Embeddings cached in Supabase")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Embedding pre-computation failed: {e}")
    
    # Return structured data
    result = {
        'video_info': video_info,
        'transcript': transcription['text'],
        'transcript_segments': transcription['segments'],
        'comments': comments,
    }
    
    # Save to cache
    try:
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False) # indent=2? Save space for cache
    except Exception as e:
        if verbose:
            print(f"   ‚ö†Ô∏è Failed to write cache: {e}")
            
    return result


# ============================================================
# Standalone CLI mode (for backwards compatibility)
# ============================================================

def format_timestamp(seconds: float) -> str:
    """Format seconds to HH:MM:SS."""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    if hours > 0:
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"
    return f"{minutes:02d}:{secs:02d}"


def is_question(text: str) -> bool:
    """Check if a comment text appears to be a question."""
    text_lower = text.lower().strip()
    if '?' in text:
        return True
    question_words = ['what', 'why', 'how', 'when', 'where', 'who', 'which', 
                      'can', 'could', 'would', 'should', 'is', 'are', 'do', 
                      'does', 'did', 'will', 'have', 'has']
    for word in question_words:
        if text_lower.startswith(word + ' '):
            return True
    return False


def export_markdown(output_path: Path, video_info: dict, transcription: dict, questions: list):
    """Export results as Markdown file."""
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(f"# {video_info['title']}\n\n")
        f.write(f"**Channel:** {video_info['channel']}  \n")
        f.write(f"**Video ID:** {video_info['id']}  \n")
        if video_info.get('upload_date'):
            upload_date = video_info['upload_date']
            formatted_date = f"{upload_date[:4]}-{upload_date[4:6]}-{upload_date[6:]}"
            f.write(f"**Upload Date:** {formatted_date}  \n")
        if video_info.get('view_count'):
            f.write(f"**Views:** {video_info['view_count']:,}  \n")
        f.write(f"**Language:** {transcription.get('language', 'unknown')}  \n")
        f.write(f"**Processed:** {datetime.now().strftime('%Y-%m-%d %H:%M')}  \n")
        f.write("\n---\n\n")
        
        f.write("## üìù Transcript\n\n")
        for segment in transcription.get('segments', []):
            timestamp = format_timestamp(segment['start'])
            f.write(f"**[{timestamp}]** {segment['text']}\n\n")
        if not transcription.get('segments'):
            f.write(transcription.get('text', 'No transcript available.'))
            f.write("\n\n")
        
        f.write("---\n\n")
        f.write("## üí≠ Viewer Sentiment & Questions\n\n")
        
        if questions:
            f.write(f"*{len(questions)} viewer questions identified (sorted by engagement):*\n\n")
            for i, q in enumerate(questions, 1):
                f.write(f"### Q{i}. {q['author']}\n")
                f.write(f"> {q['text']}\n\n")
                f.write(f"üëç {q['likes']} likes\n\n")
        else:
            f.write("*No viewer questions found in the comments.*\n\n")


def main():
    parser = argparse.ArgumentParser(
        description="YouTube Ingest Manager - Download, transcribe, and analyze videos.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument('url', help='YouTube video URL')
    parser.add_argument('--model', '-m', default='tiny',
                        choices=['tiny', 'base', 'small', 'medium', 'large'],
                        help='Whisper model size (default: tiny)')
    
    args = parser.parse_args()
    
    api_key = os.getenv('YOUTUBE_API_KEY')
    if not api_key:
        print("‚ùå YOUTUBE_API_KEY not found in environment.")
        sys.exit(1)
    
    try:
        # Use the new process_video function
        result = process_video(args.url, api_key, args.model)
        
        # Filter for questions (for standalone mode)
        questions = [c for c in result['comments'] if is_question(c['text'])]
        questions.sort(key=lambda x: x['likes'], reverse=True)
        questions = questions[:50]
        
        # Export
        script_dir = Path(__file__).parent.resolve()
        data_dir = script_dir / "data"
        data_dir.mkdir(exist_ok=True)
        
        filename = sanitize_filename(result['video_info']['title']) + ".md"
        output_path = data_dir / filename
        
        transcription = {
            'text': result['transcript'],
            'segments': result['transcript_segments'],
            'language': 'auto',
        }
        export_markdown(output_path, result['video_info'], transcription, questions)
        
        print(f"\nüéâ Done! Output saved to:")
        print(f"   {output_path}")
        
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
    